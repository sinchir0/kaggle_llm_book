# Kaggle ではじめる大規模言語モデル入門　～自然言語処理〈実践〉プログラミング～

- 『Kaggle ではじめる大規模言語モデル入門　～自然言語処理〈実践〉プログラミング～』（講談社）のサンプルコードです。
- [参考文献](references.md)・[正誤表](errata.md)・[プロフィール一覧](profiles.md)も掲載しています。
- ご感想・ご質問は、issue にてお願いします。

## 新着情報

- 2026 年 1 月 27 日：電子版が 2 月 6 日から配信されます。
- 2026 年 1 月 22 日：電子版は 2 月に配信予定です。配信日が決まり次第、[講談社サイエンティフィクの X](https://x.com/kspub_kodansha) でお知らせします。
- 2026 年 1 月 16 日：書籍が出版されました。

## サンプルコード

3 章では、主催者の了承を得て Kaggle 上にアップロードした「atmaCup #17」の[データセット](https://www.kaggle.com/datasets/takaito/atmacup17)を利用します。
「atmaCup #17」のデータセットは、[CC0 ライセンスのデータセット](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews)から作成されています。

### 3. 「atmaCup #17」に挑戦

| 節・項 | 2025-09（執筆時） | 2026-01（初版公開時） |
| --- | :---: | :---: |
| 3.2 探索的データ解析 | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-2-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-2-2026-01) |
| 3.3 単語の頻度情報を用いたモデル | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-3-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-3-2026-01) |
| 3.4 BERT 系統のモデル | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-4-bert-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-4-bert-2026-01) |
| 3.4.5 Kaggle への架け橋 | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-4-5-kaggle-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-4-5-kaggle-2026-01) |
| 3.5 大規模言語モデル | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-5-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-5-2026-01) |
| 3.6 アンサンブル | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-6-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/takaito/kaggle-llm-book-3-6-2026-01) |

### 4. 大規模言語モデルの性能改善

| 節 | 2025-09（執筆時） | 2026-01（初版公開時） |
| --- | :---: | :---: |
| 4.1 ファインチューニング | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-4-1-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-4-1-2026-01) |
| 4.2 プロンプトエンジニアリング | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-4-2-2025-09)| [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-4-2-2026-01) |

### 5. 大規模言語モデルの軽量化・高速化・省メモリ化 

| 節・項 | 2025-09（執筆時） | 2026-01（初版公開時） |
| --- | :---: | :---: |
| 5.1.2 低精度化の手法とライブラリ | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-1-2-2025-09)| [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-1-2-2026-01) |
| 5.1.3 量子化の手法とライブラリ | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-1-3-2025-09)| [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-1-3-2026-01) |
| 5.2 知識蒸留 | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sishihara/kaggle-llm-book-5-2-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sishihara/kaggle-llm-book-5-2-2026-01) |
| 5.3.2 KV キャッシュ | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-3-2-kv-2025-09)  | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-3-2-kv-2026-01) |
| 5.3.4 vLLM | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-3-4-vllm-2025-09) | [![Kaggle](https://www.kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/sinchir0/kaggle-llm-book-5-3-4-vllm-2026-01)|

## Notebook の使い方

### Copy & Edit

『Python ではじめる Kaggle スタートブック』（講談社）の付録として、公開されている Notebook を複製して編集（Copy & Edit）する方法を解説する [YouTube 動画](https://youtu.be/u6Bc0jiWu38?si=hXXoX0cuPqZjiBCa)を公開しています。

### 事前準備（4.2 節、5.1 節、5.3 節のみ）

#### Hugging Face のアクセストークンの設定

1. Hugging Face のサイトにログイン（必要に応じてユーザ登録）
1. アクセストークンの[新規発行](https://huggingface.co/settings/tokens)
    - 「Read access to contents of all public gated repos you can access」にチェックを入れる
1. Kaggle Notebook の編集画面上部「Add-ons」->「Secrets」->「Add Secret」で、発行したアクセストークンを `HF_TOKEN` として登録

#### モデルの利用規約に同意

1. 利用するモデルの Hugging Face のページに移動
    - 4.2 節：`google/gemma-2-2b-jpn-it` の[ページ](https://huggingface.co/google/gemma-2-2b-jpn-it)
    - 5.1 節、5.3 節：`google/gemma-3-1b-it` の[ページ](https://huggingface.co/google/gemma-3-1b-it)
1. 「Access Gemma on Hugging Face」で、利用規約に同意。利用規約は共通なので、両方のモデルが利用可能に。
